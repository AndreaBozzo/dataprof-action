name: 'DataProf Data Quality Assessment'
description: 'Analyze CSV files for data quality with dataprof. Comprehensive quality metrics and quality gates for CI/CD workflows.'
author: 'Andrea Bozzo'

branding:
  icon: 'database'
  color: 'blue'

inputs:
  file:
    description: 'Path to the CSV file to analyze for data quality'
    required: true

  quality-threshold:
    description: 'Overall quality score threshold (0-100). Job fails if score is below this value'
    required: false
    default: '80'

  fail-on-issues:
    description: 'Whether to fail the job if quality score is below threshold'
    required: false
    default: 'true'

  output-format:
    description: 'Output format for analysis results (json, csv, text)'
    required: false
    default: 'json'

  dataprof-version:
    description: 'Version of dataprof to use (latest, or specific version like v0.4.6)'
    required: false
    default: 'latest'

outputs:
  quality-score:
    description: 'Overall data quality score (0-100)'
    value: ${{ steps.quality-analysis.outputs.quality-score }}

  quality-level:
    description: 'Quality level classification (EXCELLENT, GOOD, FAIR, POOR)'
    value: ${{ steps.quality-analysis.outputs.quality-level }}

  completeness-score:
    description: 'Data completeness score (0-100)'
    value: ${{ steps.quality-analysis.outputs.completeness-score }}

  uniqueness-score:
    description: 'Data uniqueness score (0-100)'
    value: ${{ steps.quality-analysis.outputs.uniqueness-score }}

  validity-score:
    description: 'Data validity score (0-100)'
    value: ${{ steps.quality-analysis.outputs.validity-score }}

  consistency-score:
    description: 'Data consistency score (0-100)'
    value: ${{ steps.quality-analysis.outputs.consistency-score }}

  timeliness-score:
    description: 'Data timeliness score (0-100)'
    value: ${{ steps.quality-analysis.outputs.timeliness-score }}

  accuracy-score:
    description: 'Data accuracy score (0-100)'
    value: ${{ steps.quality-analysis.outputs.accuracy-score }}

  issues-count:
    description: 'Total number of quality issues detected'
    value: ${{ steps.quality-analysis.outputs.issues-count }}

  file-path:
    description: 'Path of the analyzed file'
    value: ${{ steps.quality-analysis.outputs.file-path }}

  analysis-summary:
    description: 'Human-readable analysis summary'
    value: ${{ steps.quality-analysis.outputs.analysis-summary }}

runs:
  using: 'composite'
  steps:
  - name: Validate inputs
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Validating inputs"

      # Validate file existence and readability
      if [[ ! -f "${{ inputs.file }}" ]]; then
        echo "::error file=${{ inputs.file }}::File not found"
        exit 1
      fi

      if [[ ! -r "${{ inputs.file }}" ]]; then
        echo "::error file=${{ inputs.file }}::File is not readable"
        exit 1
      fi

      # Check file extension (warning only)
      if [[ ! "${{ inputs.file }}" =~ \.(csv|CSV)$ ]]; then
        echo "::warning file=${{ inputs.file }}::File does not have .csv extension, proceeding anyway"
      fi

      # Validate threshold is numeric and in range
      threshold="${{ inputs.quality-threshold }}"
      if ! [[ "$threshold" =~ ^[0-9]+$ ]] || [[ "$threshold" -lt 0 ]] || [[ "$threshold" -gt 100 ]]; then
        echo "::error::quality-threshold must be a number between 0 and 100, got: $threshold"
        exit 1
      fi

      # Validate output format
      format="${{ inputs.output-format }}"
      if [[ ! "$format" =~ ^(json|csv|text)$ ]]; then
        echo "::error::output-format must be one of: json, csv, text. Got: $format"
        exit 1
      fi

      echo "Input validation passed"
      echo "::endgroup::"

  - name: Setup dataprof
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Setting up dataprof"

      # Determine version to install
      VERSION="${{ inputs.dataprof-version }}"
      if [[ "$VERSION" == "latest" ]] || [[ -z "$VERSION" ]]; then
        # Query crates.io API for latest version
        VERSION=$(curl -s https://crates.io/api/v1/crates/dataprof | jq -r '.crate.max_version')
      fi

      echo "::notice::Installing dataprof version: $VERSION"

      # Setup Rust environment if needed
      if ! command -v cargo &> /dev/null; then
        echo "::group::Installing Rust toolchain"
        curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --quiet
        echo "$HOME/.cargo/bin" >> "$GITHUB_PATH"
        source "$HOME/.cargo/env"
        echo "::endgroup::"
      fi

      # Install dataprof with proper error handling
      echo "::group::Installing dataprof via cargo"

      if ! cargo install dataprof --version "=$VERSION" --features minimal --locked; then
        echo "::warning::Failed to install with lockfile, retrying without..."
        if ! cargo install dataprof --version "=$VERSION" --features minimal; then
          echo "::warning::Failed to install specific version, trying latest..."
          if ! cargo install dataprof --features minimal; then
            echo "::error::All installation attempts failed"
            exit 1
          fi
        fi
      fi

      echo "::endgroup::"

      # Verify installation - binary is named 'dataprof-cli'
      if command -v dataprof-cli &> /dev/null; then
        echo "âœ… dataprof-cli installed successfully"
        # Add cargo bin to PATH for subsequent steps
        echo "$HOME/.cargo/bin" >> "$GITHUB_PATH"

        INSTALLED_BIN=$(which dataprof-cli)
        echo "::notice::Binary location: $INSTALLED_BIN"

        # Show version
        dataprof-cli --version || echo "::warning::Could not get version"
      else
        echo "::error::dataprof-cli installation verification failed"
        echo "::notice::Checking for alternative binary names..."
        command -v dataprof && echo "Found 'dataprof' binary" || echo "No dataprof binary found"
        ls -la "$HOME/.cargo/bin/" 2>/dev/null || echo "Cargo bin directory not accessible"
        exit 1
      fi

      echo "::endgroup::"

  - name: Run data quality analysis
    id: quality-analysis
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Running data quality analysis"

      # Setup temporary workspace with cleanup trap
      temp_dir=$(mktemp -d)
      output_file="$temp_dir/quality_analysis.${{ inputs.output-format }}"

      cleanup() {
        [[ -d "$temp_dir" ]] && rm -rf "$temp_dir"
      }
      trap cleanup EXIT

      # Build command array - dataprof uses 'analyze' subcommand
      cmd_args=(
        "analyze"
        "${{ inputs.file }}"
        "--format" "${{ inputs.output-format }}"
      )

      # Add detailed flag for comprehensive output
      if [[ "${{ inputs.output-format }}" == "json" ]]; then
        cmd_args+=("--detailed")
      fi

      # Determine dataprof binary name
      # The binary is always named 'dataprof-cli' (with .exe on Windows)
      if [[ "$RUNNER_OS" == "Windows" ]] || [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]]; then
        DATAPROF_BIN="dataprof-cli.exe"
      else
        DATAPROF_BIN="dataprof-cli"
      fi

      echo "::notice::Executing: $DATAPROF_BIN ${cmd_args[*]}"

      if ! "$DATAPROF_BIN" "${cmd_args[@]}" > "$output_file" 2>&1; then
        exit_code=$?
        echo "::error::Analysis failed with exit code $exit_code"
        echo "::group::Error output"
        cat "$output_file" 2>/dev/null || echo "No output available"
        echo "::endgroup::"
        exit $exit_code
      fi

      echo "Analysis completed successfully"
      echo "::endgroup::"

      echo "::group::Parsing analysis results"

      # Parse based on output format
      if [[ "${{ inputs.output-format }}" == "json" ]]; then
        # Ensure jq is available (GitHub runners have it pre-installed)
        if ! command -v jq &> /dev/null; then
          echo "::error::jq is required for JSON parsing but not available"
          exit 1
        fi

        # Validate JSON format
        if ! jq empty "$output_file" 2>/dev/null; then
          echo "::error::Invalid JSON format in output file"
          exit 1
        fi

        # Extract metrics from ACTUAL dataprof JSON structure
        # Overall quality score
        quality_score=$(jq -r '.quality.quality_score // .summary.data_quality_score // "0"' "$output_file" 2>/dev/null || echo "0")

        # Calculate quality level from score
        if awk -v score="$quality_score" 'BEGIN { exit !(score >= 90) }'; then
          quality_level="EXCELLENT"
        elif awk -v score="$quality_score" 'BEGIN { exit !(score >= 75) }'; then
          quality_level="GOOD"
        elif awk -v score="$quality_score" 'BEGIN { exit !(score >= 60) }'; then
          quality_level="FAIR"
        else
          quality_level="POOR"
        fi

        # Extract individual dimension scores from data_quality_metrics
        # Completeness score (from complete_records_ratio)
        completeness=$(jq -r '.data_quality_metrics.completeness.complete_records_ratio // "0"' "$output_file" 2>/dev/null || echo "0")

        # Uniqueness score (from key_uniqueness)
        uniqueness=$(jq -r '.data_quality_metrics.uniqueness.key_uniqueness // "0"' "$output_file" 2>/dev/null || echo "0")

        # Consistency score (from data_type_consistency)
        consistency=$(jq -r '.data_quality_metrics.consistency.data_type_consistency // "0"' "$output_file" 2>/dev/null || echo "0")

        # Accuracy score (100 - outlier_ratio)
        outlier_ratio=$(jq -r '.data_quality_metrics.accuracy.outlier_ratio // "0"' "$output_file" 2>/dev/null || echo "0")
        accuracy=$(awk -v outlier="$outlier_ratio" 'BEGIN { printf "%.2f", 100 - outlier }')

        # Timeliness score - Check if timeliness metrics exist in JSON
        if jq -e '.data_quality_metrics.timeliness' "$output_file" >/dev/null 2>&1; then
          stale_ratio=$(jq -r '.data_quality_metrics.timeliness.stale_data_ratio // "0"' "$output_file" 2>/dev/null || echo "0")
          timeliness=$(awk -v stale="$stale_ratio" 'BEGIN { printf "%.2f", 100 - stale }')
        else
          # Timeliness not available in this version, default to 100 (no timeliness issues)
          timeliness="100.0"
        fi

        # Validity is not a separate dimension in ISO 8000/25012, use consistency as proxy
        validity="$consistency"

        # Count issues from all dimensions
        duplicate_rows=$(jq -r '.data_quality_metrics.uniqueness.duplicate_rows // "0"' "$output_file" 2>/dev/null || echo "0")
        format_violations=$(jq -r '.data_quality_metrics.consistency.format_violations // "0"' "$output_file" 2>/dev/null || echo "0")
        range_violations=$(jq -r '.data_quality_metrics.accuracy.range_violations // "0"' "$output_file" 2>/dev/null || echo "0")
        issues_count=$(awk -v dup="$duplicate_rows" -v fmt="$format_violations" -v range="$range_violations" 'BEGIN { print dup + fmt + range }')

        file_path="${{ inputs.file }}"

        echo "JSON results parsed successfully"
      else
        # Basic text parsing for non-JSON formats
        if grep -q "Overall Quality Score:" "$output_file"; then
          quality_score=$(grep "Overall Quality Score:" "$output_file" | grep -o '[0-9]*\.*[0-9]*' | head -1 || echo "0")
          quality_level=$(grep "Quality Level:" "$output_file" | awk '{print $NF}' | head -1 || echo "UNKNOWN")
        else
          quality_score="0"
          quality_level="UNKNOWN"
        fi

        # Set defaults for unsupported metrics in text format
        completeness="0"
        uniqueness="0"
        validity="0"
        consistency="0"
        timeliness="0"
        accuracy="0"
        issues_count="0"
        file_path="${{ inputs.file }}"

        echo "Text results parsed"
      fi

      echo "::endgroup::"

      echo "::group::Setting outputs and environment variables"

      # Validate extracted metrics
      if [[ ! "$quality_score" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
        echo "::warning::Invalid quality score format: $quality_score, defaulting to 0"
        quality_score="0"
      fi

      # Create structured analysis summary
      analysis_summary="Data Quality Analysis Results:
      Overall Score: ${quality_score}%
      Quality Level: ${quality_level}
      Completeness: ${completeness}%
      Uniqueness: ${uniqueness}%
      Validity: ${validity}%
      Consistency: ${consistency}%
      Timeliness: ${timeliness}%
      Accuracy: ${accuracy}%
      Issues Found: ${issues_count}
      File: ${file_path}"

      # Set GitHub outputs safely
      {
        echo "quality-score=$quality_score"
        echo "quality-level=$quality_level"
        echo "completeness-score=$completeness"
        echo "uniqueness-score=$uniqueness"
        echo "validity-score=$validity"
        echo "consistency-score=$consistency"
        echo "timeliness-score=$timeliness"
        echo "accuracy-score=$accuracy"
        echo "issues-count=$issues_count"
        echo "file-path=$file_path"
      } >> "$GITHUB_OUTPUT"

      # Set multiline outputs using heredoc syntax
      {
        echo "analysis-summary<<EOF_SUMMARY"
        echo "$analysis_summary"
        echo "EOF_SUMMARY"
      } >> "$GITHUB_OUTPUT"

      # Store for threshold validation
      {
        echo "QUALITY_SCORE=$quality_score"
        echo "THRESHOLD=${{ inputs.quality-threshold }}"
      } >> "$GITHUB_ENV"

      echo "::endgroup::"

      # Display summary with GitHub annotations
      echo "::notice title=Analysis Complete::Quality Score: ${quality_score}% (Threshold: ${{ inputs.quality-threshold }}%)"

  - name: Quality gate validation
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Quality gate validation"

      # Extract values from environment with defaults
      quality_score=${QUALITY_SCORE:-0}
      threshold=${THRESHOLD:-80}
      fail_on_issues="${{ inputs.fail-on-issues }}"

      echo "::notice::Validating quality score: ${quality_score}% against threshold: ${threshold}%"

      # Robust floating point comparison using awk
      if awk -v score="$quality_score" -v thresh="$threshold" 'BEGIN { exit !(score >= thresh) }'; then
        echo "::notice title=Quality Gate Passed::Quality score (${quality_score}%) meets threshold (${threshold}%)"
        echo "Quality gate validation successful"
      else
        message="Quality score (${quality_score}%) below threshold (${threshold}%)"

        if [[ "$fail_on_issues" == "true" ]]; then
          echo "::error title=Quality Gate Failed::$message"
          echo "Quality gate validation failed - stopping workflow"
          exit 1
        else
          echo "::warning title=Quality Gate Warning::$message"
          echo "Quality gate warning - continuing workflow (fail-on-issues=false)"
        fi
      fi

      echo "::endgroup::"

  - name: Generate workflow summary
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Generating workflow summary"

      # Determine threshold status
      quality_score="${{ steps.quality-analysis.outputs.quality-score }}"
      threshold="${{ inputs.quality-threshold }}"
      threshold_status="FAILED"

      if awk -v score="$quality_score" -v thresh="$threshold" 'BEGIN { exit !(score >= thresh) }'; then
        threshold_status="PASSED"
      fi

      # Generate comprehensive summary
      output_format="${{ inputs.output-format }}"
      dataprof_version="${{ inputs.dataprof-version }}"
      fail_on_issues="${{ inputs.fail-on-issues }}"
      issues_count="${{ steps.quality-analysis.outputs.issues-count }}"
      quality_level="${{ steps.quality-analysis.outputs.quality-level }}"
      file_path="${{ steps.quality-analysis.outputs.file-path }}"

      next_steps="- Excellent! Your data quality is high
      - No critical issues detected
      - Consider monitoring data quality over time"

      if [[ "$issues_count" != "0" ]]; then
        next_steps="- Review the detailed quality metrics in the action outputs
        - Address the ${issues_count} issue(s) identified
        - Run analysis again after applying improvements"
      fi

      {
        echo "# DataProf Data Quality Analysis"
        echo ""
        echo "## Results Summary"
        echo ""
        echo "| Metric | Score |"
        echo "|--------|-------|"
        echo "| **Overall Quality** | ${quality_score}% (${quality_level}) |"
        echo "| **Completeness** | ${{ steps.quality-analysis.outputs.completeness-score }}% |"
        echo "| **Uniqueness** | ${{ steps.quality-analysis.outputs.uniqueness-score }}% |"
        echo "| **Validity** | ${{ steps.quality-analysis.outputs.validity-score }}% |"
        echo "| **Consistency** | ${{ steps.quality-analysis.outputs.consistency-score }}% |"
        echo "| **Timeliness** | ${{ steps.quality-analysis.outputs.timeliness-score }}% |"
        echo "| **Accuracy** | ${{ steps.quality-analysis.outputs.accuracy-score }}% |"
        echo ""
        echo "## Quality Gate"
        echo ""
        echo "- **Threshold**: ${threshold}%"
        echo "- **Result**: ${threshold_status}"
        echo "- **Issues Found**: ${issues_count}"
        echo "- **Fail on Issues**: ${fail_on_issues}"
        echo ""
        echo "## Analysis Details"
        echo ""
        echo "- **File**: \`${file_path}\`"
        echo "- **Output Format**: ${output_format}"
        echo "- **DataProf Version**: ${dataprof_version}"
        echo ""
        echo "## Next Steps"
        echo ""
        echo "${next_steps}"
        echo ""
        echo "---"
        echo "*Powered by [DataProf v${dataprof_version}](https://github.com/AndreaBozzo/dataprof) â€¢ Generated $(date -u '+%Y-%m-%d %H:%M:%S UTC')*"
      } >> "$GITHUB_STEP_SUMMARY"

      echo "Workflow summary generated"
      echo "::endgroup::"