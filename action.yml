name: 'DataProf ML Readiness Assessment'
description: 'Analyze CSV files for ML readiness with dataprof and integrate quality gates into CI/CD workflows'
author: 'Andrea Bozzo'

branding:
  icon: 'database'
  color: 'blue'

inputs:
  file:
    description: 'Path to the CSV file to analyze for ML readiness'
    required: true

  ml-threshold:
    description: 'ML readiness score threshold (0-100). Job fails if score is below this value'
    required: false
    default: '80'

  fail-on-issues:
    description: 'Whether to fail the job if ML readiness score is below threshold'
    required: false
    default: 'true'

  output-format:
    description: 'Output format for analysis results (json, csv, text)'
    required: false
    default: 'json'

  generate-code:
    description: 'Generate actionable preprocessing code snippets'
    required: false
    default: 'true'

  dataprof-version:
    description: 'Version of dataprof to use (latest, or specific version like v0.4.6)'
    required: false
    default: 'latest'

outputs:
  ml-score:
    description: 'Overall ML readiness score (0-100)'
    value: ${{ steps.ml-analysis.outputs.ml-score }}

  readiness-level:
    description: 'ML readiness level classification (HIGH, MEDIUM, LOW)'
    value: ${{ steps.ml-analysis.outputs.readiness-level }}

  completeness-score:
    description: 'Data completeness score'
    value: ${{ steps.ml-analysis.outputs.completeness-score }}

  consistency-score:
    description: 'Data consistency score'
    value: ${{ steps.ml-analysis.outputs.consistency-score }}

  recommendations-count:
    description: 'Number of ML improvement recommendations'
    value: ${{ steps.ml-analysis.outputs.recommendations-count }}

  recommendations:
    description: 'JSON array of ML readiness recommendations'
    value: ${{ steps.ml-analysis.outputs.recommendations }}

  code-snippets:
    description: 'Generated preprocessing code snippets'
    value: ${{ steps.ml-analysis.outputs.code-snippets }}

  analysis-summary:
    description: 'Human-readable analysis summary'
    value: ${{ steps.ml-analysis.outputs.analysis-summary }}

runs:
  using: 'composite'
  steps:
    - name: Validate inputs
      shell: bash
      run: |
        echo "Validating inputs..."

        # Check if file exists
        if [[ ! -f "${{ inputs.file }}" ]]; then
          echo "ERROR: File '${{ inputs.file }}' not found"
          exit 1
        fi

        # Validate threshold
        threshold="${{ inputs.ml-threshold }}"
        if ! [[ "$threshold" =~ ^[0-9]+$ ]] || [[ "$threshold" -lt 0 ]] || [[ "$threshold" -gt 100 ]]; then
          echo "ERROR: ml-threshold must be a number between 0 and 100"
          exit 1
        fi

        # Check file extension
        if [[ ! "${{ inputs.file }}" =~ \.(csv|CSV)$ ]]; then
          echo "WARNING: File does not have .csv extension, proceeding anyway..."
        fi

        echo "Input validation passed"

    - name: Setup dataprof
      shell: bash
      run: |
        echo "Setting up dataprof..."

        # Install via cargo for now (binary downloads to be implemented later)
        if ! command -v cargo &> /dev/null; then
          echo "Installing Rust and Cargo..."
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
          source "$HOME/.cargo/env"
        fi

        echo "Installing dataprof via cargo..."
        cargo install dataprof --features minimal --locked || {
          echo "Failed to install specific version, trying latest..."
          cargo install dataprof --features minimal
        }

        # Verify installation
        if command -v dataprof-cli &> /dev/null; then
          echo "dataprof-cli is ready"
        else
          echo "ERROR: dataprof setup failed"
          exit 1
        fi

    - name: Run ML readiness analysis
      id: ml-analysis
      shell: bash
      run: |
        echo "Running ML readiness analysis..."

        # Set up temporary files
        temp_dir=$(mktemp -d)
        output_file="$temp_dir/ml_analysis.json"

        # Build command
        cmd_args=()
        cmd_args+=("${{ inputs.file }}")
        cmd_args+=("--quality")  # Required for ML analysis
        cmd_args+=("--ml-score")
        cmd_args+=("--format" "${{ inputs.output-format }}")

        if [[ "${{ inputs.generate-code }}" == "true" ]]; then
          cmd_args+=("--ml-code")
        fi

        # Run analysis
        echo "Executing: dataprof-cli ${cmd_args[*]}"

        dataprof-cli "${cmd_args[@]}" > "$output_file" 2>&1 || {
          echo "ERROR: Analysis failed with exit code $?"
          echo "Output:"
          cat "$output_file" || true
          exit 1
        }

        echo "Analysis completed successfully"

        # Parse results based on format
        if [[ "${{ inputs.output-format }}" == "json" ]]; then
          if command -v jq &> /dev/null; then
            echo "Parsing JSON results with jq..."

            # Extract key metrics using correct JSON structure
            ml_score=$(jq -r '.summary.ml_readiness.score // "0"' "$output_file" 2>/dev/null || echo "0")
            readiness_level=$(jq -r '.summary.ml_readiness.level // "UNKNOWN"' "$output_file" 2>/dev/null || echo "UNKNOWN")
            completeness=$(jq -r '.summary.completeness_score // "0"' "$output_file" 2>/dev/null || echo "0")
            consistency=$(jq -r '.summary.consistency_score // "0"' "$output_file" 2>/dev/null || echo "0")

            # Count recommendations
            rec_count=$(jq -r '.summary.ml_readiness.recommendations // [] | length' "$output_file" 2>/dev/null || echo "0")

            # Get recommendations as JSON string
            recommendations=$(jq -c '.summary.ml_readiness.recommendations // []' "$output_file" 2>/dev/null || echo "[]")

            # Get code snippets if available (extract from recommendations)
            code_snippets=$(jq -c '[.summary.ml_readiness.recommendations[]? | select(.code_snippet) | {category, code_snippet}] // []' "$output_file" 2>/dev/null || echo "[]")

          else
            echo "jq not available, installing..."
            # Install jq based on OS
            case "$OSTYPE" in
              linux*)
                if command -v apt-get &> /dev/null; then
                  sudo apt-get update && sudo apt-get install -y jq
                elif command -v yum &> /dev/null; then
                  sudo yum install -y jq
                elif command -v apk &> /dev/null; then
                  sudo apk add jq
                fi
                ;;
              darwin*)
                if command -v brew &> /dev/null; then
                  brew install jq
                fi
                ;;
            esac

            # Retry parsing with jq
            if command -v jq &> /dev/null; then
              ml_score=$(jq -r '.summary.ml_readiness.score // "0"' "$output_file" 2>/dev/null || echo "0")
              readiness_level=$(jq -r '.summary.ml_readiness.level // "UNKNOWN"' "$output_file" 2>/dev/null || echo "UNKNOWN")
              completeness=$(jq -r '.summary.completeness_score // "0"' "$output_file" 2>/dev/null || echo "0")
              consistency=$(jq -r '.summary.consistency_score // "0"' "$output_file" 2>/dev/null || echo "0")
              rec_count=$(jq -r '.summary.ml_readiness.recommendations // [] | length' "$output_file" 2>/dev/null || echo "0")
              recommendations=$(jq -c '.summary.ml_readiness.recommendations // []' "$output_file" 2>/dev/null || echo "[]")
              code_snippets=$(jq -c '[.summary.ml_readiness.recommendations[]? | select(.code_snippet) | {category, code_snippet}] // []' "$output_file" 2>/dev/null || echo "[]")
            else
              echo "jq installation failed, using basic parsing..."
              # Basic parsing fallback
              if grep -q '"score"' "$output_file"; then
                ml_score=$(grep '"score"' "$output_file" | tail -1 | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "0")
              else
                ml_score="0"
              fi
              readiness_level="UNKNOWN"
              completeness="0"
              consistency="0"
              rec_count="0"
              recommendations='[]'
              code_snippets='[]'
            fi
          fi
        else
          echo "Parsing text/csv output..."
          # For non-JSON formats, try to extract from text
          if grep -q "Overall Score:" "$output_file"; then
            ml_score=$(grep "Overall Score:" "$output_file" | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "0")
            readiness_level=$(grep "Overall Score:" "$output_file" | grep -o 'Ready\|Needs Work\|Not Ready' | head -1 || echo "UNKNOWN")
          else
            ml_score="0"
            readiness_level="UNKNOWN"
          fi
          completeness="0"
          consistency="0"
          rec_count="0"
          recommendations='[]'
          code_snippets='[]'
        fi

        # Create analysis summary
        analysis_summary="ML Readiness Analysis Results:
        Overall Score: ${ml_score}%
        Readiness Level: ${readiness_level}
        Completeness: ${completeness}%
        Consistency: ${consistency}%
        Recommendations: ${rec_count} items"

        # Set outputs
        echo "ml-score=$ml_score" >> $GITHUB_OUTPUT
        echo "readiness-level=$readiness_level" >> $GITHUB_OUTPUT
        echo "completeness-score=$completeness" >> $GITHUB_OUTPUT
        echo "consistency-score=$consistency" >> $GITHUB_OUTPUT
        echo "recommendations-count=$rec_count" >> $GITHUB_OUTPUT
        echo "recommendations<<EOF" >> $GITHUB_OUTPUT
        echo "$recommendations" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT
        echo "code-snippets<<EOF" >> $GITHUB_OUTPUT
        echo "$code_snippets" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT
        echo "analysis-summary<<EOF" >> $GITHUB_OUTPUT
        echo "$analysis_summary" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT

        # Store values for threshold check
        echo "ML_SCORE=$ml_score" >> $GITHUB_ENV
        echo "THRESHOLD=${{ inputs.ml-threshold }}" >> $GITHUB_ENV

        # Display results
        echo ""
        echo "Analysis Results:"
        echo "   Score: ${ml_score}%"
        echo "   Level: ${readiness_level}"
        echo "   Threshold: ${{ inputs.ml-threshold }}%"
        echo "   Recommendations: ${rec_count}"
        echo ""

        # Cleanup
        rm -rf "$temp_dir"

    - name: Check ML readiness threshold
      shell: bash
      run: |
        echo "Checking ML readiness threshold..."

        ml_score=${ML_SCORE:-0}
        threshold=${THRESHOLD:-80}
        fail_on_issues="${{ inputs.fail-on-issues }}"

        echo "ML Score: ${ml_score}%"
        echo "Threshold: ${threshold}%"
        echo "Fail on issues: ${fail_on_issues}"

        # Use awk for floating point comparison
        if awk "BEGIN {exit !($ml_score < $threshold)}"; then
          echo "ML readiness score (${ml_score}%) is below threshold (${threshold}%)"

          if [[ "$fail_on_issues" == "true" ]]; then
            echo "Failing job due to low ML readiness score"
            echo "::error title=Low ML Readiness Score::ML readiness score (${ml_score}%) is below threshold (${threshold}%)"
            exit 1
          else
            echo "ML readiness score is low but continuing (fail-on-issues=false)"
            echo "::warning title=Low ML Readiness Score::ML readiness score (${ml_score}%) is below threshold (${threshold}%)"
          fi
        else
          echo "ML readiness score meets threshold requirements"
        fi

    - name: Generate workflow summary
      shell: bash
      run: |
        echo "Generating workflow summary..."

        cat >> $GITHUB_STEP_SUMMARY << 'EOF'
        # DataProf ML Readiness Analysis

        ## Results Summary

        | Metric | Score | Status |
        |--------|-------|--------|
        | **ML Readiness** | ${{ steps.ml-analysis.outputs.ml-score }}% | ${{ steps.ml-analysis.outputs.readiness-level }} |
        | **Completeness** | ${{ steps.ml-analysis.outputs.completeness-score }}% | - |
        | **Consistency** | ${{ steps.ml-analysis.outputs.consistency-score }}% | - |
        | **Recommendations** | ${{ steps.ml-analysis.outputs.recommendations-count }} | - |

        ## Threshold Check
        - **Threshold**: ${{ inputs.ml-threshold }}%
        - **Result**: ${{ steps.ml-analysis.outputs.ml-score >= inputs.ml-threshold && 'PASSED' || 'FAILED' }}

        ## File Analysis
        - **File**: `${{ inputs.file }}`
        - **Format**: ${{ inputs.output-format }}
        - **Generated Code**: ${{ inputs.generate-code == 'true' && 'Yes' || 'No' }}

        ## Recommendations
        ${{ steps.ml-analysis.outputs.recommendations-count > 0 && 'See action outputs for detailed recommendations and code snippets.' || 'No specific recommendations - data looks good for ML!' }}

        ---
        *Analysis powered by [dataprof](https://github.com/AndreaBozzo/dataprof)*
        EOF

        echo "Summary generated"