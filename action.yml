name: 'DataProf ML Readiness Assessment'
description: 'Analyze CSV files for ML readiness with dataprof and integrate quality gates into CI/CD workflows'
author: 'Andrea Bozzo'

branding:
  icon: 'database'
  color: 'blue'

inputs:
  file:
    description: 'Path to the CSV file to analyze for ML readiness'
    required: true

  ml-threshold:
    description: 'ML readiness score threshold (0-100). Job fails if score is below this value'
    required: false
    default: '80'

  fail-on-issues:
    description: 'Whether to fail the job if ML readiness score is below threshold'
    required: false
    default: 'true'

  output-format:
    description: 'Output format for analysis results (json, csv, text)'
    required: false
    default: 'json'

  generate-code:
    description: 'Generate actionable preprocessing code snippets'
    required: false
    default: 'true'

  dataprof-version:
    description: 'Version of dataprof to use (latest, or specific version like v0.4.6)'
    required: false
    default: 'latest'

outputs:
  ml-score:
    description: 'Overall ML readiness score (0-100)'
    value: ${{ steps.ml-analysis.outputs.ml-score }}

  readiness-level:
    description: 'ML readiness level classification (HIGH, MEDIUM, LOW)'
    value: ${{ steps.ml-analysis.outputs.readiness-level }}

  completeness-score:
    description: 'Data completeness score'
    value: ${{ steps.ml-analysis.outputs.completeness-score }}

  consistency-score:
    description: 'Data consistency score'
    value: ${{ steps.ml-analysis.outputs.consistency-score }}

  recommendations-count:
    description: 'Number of ML improvement recommendations'
    value: ${{ steps.ml-analysis.outputs.recommendations-count }}

  recommendations:
    description: 'JSON array of ML readiness recommendations'
    value: ${{ steps.ml-analysis.outputs.recommendations }}

  code-snippets:
    description: 'Generated preprocessing code snippets'
    value: ${{ steps.ml-analysis.outputs.code-snippets }}

  analysis-summary:
    description: 'Human-readable analysis summary'
    value: ${{ steps.ml-analysis.outputs.analysis-summary }}

runs:
  using: 'composite'
  steps:
    - name: Validate inputs
      shell: bash
      run: |
        set -euo pipefail

        echo "::group::Validating inputs"

        # Validate file existence and readability
        if [[ ! -f "${{ inputs.file }}" ]]; then
          echo "::error file=${{ inputs.file }}::File not found"
          exit 1
        fi

        if [[ ! -r "${{ inputs.file }}" ]]; then
          echo "::error file=${{ inputs.file }}::File is not readable"
          exit 1
        fi

        # Validate threshold is numeric and in range
        threshold="${{ inputs.ml-threshold }}"
        if ! [[ "$threshold" =~ ^[0-9]+$ ]] || [[ "$threshold" -lt 0 ]] || [[ "$threshold" -gt 100 ]]; then
          echo "::error::ml-threshold must be a number between 0 and 100, got: $threshold"
          exit 1
        fi

        # Validate output format
        format="${{ inputs.output-format }}"
        if [[ ! "$format" =~ ^(json|csv|text)$ ]]; then
          echo "::error::output-format must be one of: json, csv, text. Got: $format"
          exit 1
        fi

        # Check file extension (warning only)
        if [[ ! "${{ inputs.file }}" =~ \.(csv|CSV)$ ]]; then
          echo "::warning file=${{ inputs.file }}::File does not have .csv extension, proceeding anyway"
        fi

        echo "âœ“ Input validation passed"
        echo "::endgroup::"

    - name: Setup dataprof
      shell: bash
      run: |
        set -euo pipefail

        echo "::group::Setting up dataprof"

        # Determine version to install
        VERSION="${{ inputs.dataprof-version }}"
        if [[ "$VERSION" == "latest" ]]; then
          VERSION="0.4.6"
        fi

        echo "::notice::Installing dataprof version: $VERSION"

        # Setup Rust environment if needed
        if ! command -v cargo &> /dev/null; then
          echo "::group::Installing Rust toolchain"
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --quiet
          echo "$HOME/.cargo/bin" >> "$GITHUB_PATH"
          source "$HOME/.cargo/env"
          echo "::endgroup::"
        fi

        # Install dataprof with proper error handling
        echo "::group::Installing dataprof via cargo"

        install_dataprof() {
          local version_spec="$1"
          local features="$2"
          local locked="$3"

          local cmd="cargo install dataprof --version '$version_spec' --features '$features'"
          if [[ "$locked" == "true" ]]; then
            cmd="$cmd --locked"
          fi

          echo "Running: $cmd"
          eval "$cmd" 2>&1
        }

        # Try installation with fallback strategy
        if ! install_dataprof "=$VERSION" "minimal" "true"; then
          echo "::warning::Failed to install with lockfile, retrying without..."
          if ! install_dataprof "=$VERSION" "minimal" "false"; then
            echo "::warning::Failed to install specific version, trying latest..."
            if ! install_dataprof "*" "minimal" "false"; then
              echo "::error::All installation attempts failed"
              exit 1
            fi
          fi
        fi

        echo "::endgroup::"

        # Verify installation and add to PATH
        if command -v dataprof-cli &> /dev/null; then
          echo "âœ“ dataprof-cli installed successfully"
          echo "$HOME/.cargo/bin" >> "$GITHUB_PATH"
        else
          echo "::error::dataprof-cli installation verification failed"
          exit 1
        fi

        echo "::endgroup::"

    - name: Run ML readiness analysis
      id: ml-analysis
      shell: bash
      run: |
        set -euo pipefail

        echo "::group::Running ML readiness analysis"

        # Setup temporary workspace with cleanup trap
        temp_dir=$(mktemp -d)
        output_file="$temp_dir/ml_analysis.${{ inputs.output-format }}"

        cleanup() {
          [[ -d "$temp_dir" ]] && rm -rf "$temp_dir"
        }
        trap cleanup EXIT

        # Build command array
        cmd_args=(
          "${{ inputs.file }}"
          "--quality"
          "--ml-score"
          "--format" "${{ inputs.output-format }}"
        )

        if [[ "${{ inputs.generate-code }}" == "true" ]]; then
          cmd_args+=("--ml-code")
        fi

        # Execute analysis with proper error handling
        echo "::notice::Executing: dataprof-cli ${cmd_args[*]}"

        if ! dataprof-cli "${cmd_args[@]}" > "$output_file" 2>&1; then
          exit_code=$?
          echo "::error::Analysis failed with exit code $exit_code"
          echo "::group::Error output"
          cat "$output_file" 2>/dev/null || echo "No output available"
          echo "::endgroup::"
          exit $exit_code
        fi

        echo "âœ“ Analysis completed successfully"
        echo "::endgroup::"

        echo "::group::Parsing analysis results"

        # Define functions for parsing
        parse_json_results() {
          local file="$1"

          # Ensure jq is available (GitHub runners have it pre-installed)
          if ! command -v jq &> /dev/null; then
            echo "::error::jq is required for JSON parsing but not available"
            return 1
          fi

          # Validate JSON format
          if ! jq empty "$file" 2>/dev/null; then
            echo "::error::Invalid JSON format in output file"
            return 1
          fi

          # Extract metrics with proper error handling
          ml_score=$(jq -r '.summary.ml_readiness.score // "0"' "$file" 2>/dev/null || echo "0")
          readiness_level=$(jq -r '.summary.ml_readiness.level // "UNKNOWN"' "$file" 2>/dev/null || echo "UNKNOWN")
          completeness=$(jq -r '.summary.completeness_score // "0"' "$file" 2>/dev/null || echo "0")
          consistency=$(jq -r '.summary.consistency_score // "0"' "$file" 2>/dev/null || echo "0")
          rec_count=$(jq -r '.summary.ml_readiness.recommendations // [] | length' "$file" 2>/dev/null || echo "0")
          recommendations=$(jq -c '.summary.ml_readiness.recommendations // []' "$file" 2>/dev/null || echo "[]")
          code_snippets=$(jq -c '[.summary.ml_readiness.recommendations[]? | select(.code_snippet) | {category, code_snippet}] // []' "$file" 2>/dev/null || echo "[]")
        }

        parse_text_results() {
          local file="$1"

          # Basic text parsing for non-JSON formats
          if grep -q "Overall Score:" "$file"; then
            ml_score=$(grep "Overall Score:" "$file" | grep -o '[0-9]*\.*[0-9]*' | head -1 || echo "0")
            readiness_level=$(grep "Overall Score:" "$file" | grep -o 'HIGH\|MEDIUM\|LOW\|Ready\|Needs Work\|Not Ready' | head -1 || echo "UNKNOWN")
          else
            ml_score="0"
            readiness_level="UNKNOWN"
          fi

          # Set defaults for unsupported metrics in text format
          completeness="0"
          consistency="0"
          rec_count="0"
          recommendations='[]'
          code_snippets='[]'
        }

        # Parse based on output format
        case "${{ inputs.output-format }}" in
          json)
            if parse_json_results "$output_file"; then
              echo "âœ“ JSON results parsed successfully"
            else
              echo "::warning::JSON parsing failed, falling back to text parsing"
              parse_text_results "$output_file"
            fi
            ;;
          *)
            parse_text_results "$output_file"
            echo "âœ“ Text results parsed"
            ;;
        esac

        echo "::endgroup::"

        echo "::group::Setting outputs and environment variables"

        # Validate extracted metrics
        if [[ ! "$ml_score" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
          echo "::warning::Invalid ML score format: $ml_score, defaulting to 0"
          ml_score="0"
        fi

        # Create structured analysis summary
        analysis_summary=$(cat <<EOF
ML Readiness Analysis Results:
ðŸ“Š Overall Score: ${ml_score}%
ðŸŽ¯ Readiness Level: ${readiness_level}
âœ… Completeness: ${completeness}%
ðŸ”„ Consistency: ${consistency}%
ðŸ’¡ Recommendations: ${rec_count} items
ðŸ“ File: ${{ inputs.file }}
EOF
        )

        # Set GitHub outputs safely
        {
          echo "ml-score=$ml_score"
          echo "readiness-level=$readiness_level"
          echo "completeness-score=$completeness"
          echo "consistency-score=$consistency"
          echo "recommendations-count=$rec_count"
        } >> "$GITHUB_OUTPUT"

        # Set multiline outputs using heredoc syntax
        {
          echo "recommendations<<EOF_RECOMMENDATIONS"
          echo "$recommendations"
          echo "EOF_RECOMMENDATIONS"
          echo "code-snippets<<EOF_CODE"
          echo "$code_snippets"
          echo "EOF_CODE"
          echo "analysis-summary<<EOF_SUMMARY"
          echo "$analysis_summary"
          echo "EOF_SUMMARY"
        } >> "$GITHUB_OUTPUT"

        # Store for threshold validation
        {
          echo "ML_SCORE=$ml_score"
          echo "THRESHOLD=${{ inputs.ml-threshold }}"
        } >> "$GITHUB_ENV"

        echo "::endgroup::"

        # Display summary with GitHub annotations
        echo "::notice title=Analysis Complete::ML Readiness Score: ${ml_score}% (Threshold: ${{ inputs.ml-threshold }}%)"

    - name: Quality gate validation
      shell: bash
      run: |
        set -euo pipefail

        echo "::group::Quality gate validation"

        # Extract values from environment with defaults
        ml_score=${ML_SCORE:-0}
        threshold=${THRESHOLD:-80}
        fail_on_issues="${{ inputs.fail-on-issues }}"

        echo "::notice::Validating ML readiness score: ${ml_score}% against threshold: ${threshold}%"

        # Robust floating point comparison using awk
        threshold_met() {
          awk -v score="$ml_score" -v thresh="$threshold" 'BEGIN { exit !(score >= thresh) }'
        }

        if threshold_met; then
          echo "::notice title=Quality Gate Passed::âœ… ML score (${ml_score}%) meets threshold (${threshold}%)"
          echo "ðŸŽ‰ Quality gate validation successful"
        else
          message="âŒ ML score (${ml_score}%) below threshold (${threshold}%)"

          if [[ "$fail_on_issues" == "true" ]]; then
            echo "::error title=Quality Gate Failed::$message"
            echo "ðŸ’¥ Quality gate validation failed - stopping workflow"
            exit 1
          else
            echo "::warning title=Quality Gate Warning::$message"
            echo "âš ï¸ Quality gate warning - continuing workflow (fail-on-issues=false)"
          fi
        fi

        echo "::endgroup::"

    - name: Generate workflow summary
      shell: bash
      run: |
        set -euo pipefail

        echo "::group::Generating workflow summary"

        # Determine threshold status
        ml_score="${{ steps.ml-analysis.outputs.ml-score }}"
        threshold="${{ inputs.ml-threshold }}"
        threshold_status="ðŸ”´ FAILED"

        if awk -v score="$ml_score" -v thresh="$threshold" 'BEGIN { exit !(score >= thresh) }'; then
          threshold_status="âœ… PASSED"
        fi

        # Generate comprehensive summary
        cat >> "$GITHUB_STEP_SUMMARY" << EOF
# ðŸ“Š DataProf ML Readiness Analysis

## ðŸŽ¯ Results Summary

| Metric | Score | Status |
|--------|-------|--------|
| **ML Readiness** | ${{ steps.ml-analysis.outputs.ml-score }}% | \`${{ steps.ml-analysis.outputs.readiness-level }}\` |
| **Completeness** | ${{ steps.ml-analysis.outputs.completeness-score }}% | - |
| **Consistency** | ${{ steps.ml-analysis.outputs.consistency-score }}% | - |
| **Recommendations** | ${{ steps.ml-analysis.outputs.recommendations-count }} items | - |

## ðŸš¦ Quality Gate

- **Threshold**: ${threshold}%
- **Result**: ${threshold_status}
- **Action**: \`${{ inputs.fail-on-issues }}\`

## ðŸ“ File Analysis

- **File**: \`${{ inputs.file }}\`
- **Output Format**: \`${{ inputs.output-format }}\`
- **Code Generation**: \`${{ inputs.generate-code }}\`
- **DataProf Version**: \`${{ inputs.dataprof-version }}\`

## ðŸ’¡ Next Steps

${{ steps.ml-analysis.outputs.recommendations-count != '0' && '- Review the detailed recommendations in the action outputs
- Consider implementing the suggested preprocessing steps
- Run analysis again after applying improvements' || '- âœ¨ Great! Your data appears to be ML-ready
- No specific improvements needed at this time
- Consider monitoring data quality over time' }}

---
*Powered by [DataProf v${{ inputs.dataprof-version }}](https://github.com/AndreaBozzo/dataprof) â€¢ Generated $(date -u '+%Y-%m-%d %H:%M:%S UTC')*
EOF

        echo "âœ“ Workflow summary generated"
        echo "::endgroup::"