name: 'DataProf Data Quality Assessment'
description: 'Analyze CSV, JSON, and Parquet files for data quality with dataprof. Comprehensive quality metrics and quality gates for CI/CD workflows with batch processing support.'
author: 'Andrea Bozzo'

branding:
  icon: 'database'
  color: 'blue'

inputs:
  file:
    description: 'Path to file(s) to analyze. Single file path or glob pattern (e.g., data/**/*.csv). Supports CSV, JSON, and Parquet formats.'
    required: true

  batch-mode:
    description: 'Enable batch processing mode for analyzing multiple files'
    required: false
    default: 'false'

  recursive:
    description: 'Recursively process directories when batch-mode is enabled'
    required: false
    default: 'false'

  parallel:
    description: 'Enable parallel processing for batch mode (faster for multiple files)'
    required: false
    default: 'true'

  quality-threshold:
    description: 'Overall quality score threshold (0-100). Job fails if score is below this value'
    required: false
    default: '80'

  fail-on-issues:
    description: 'Whether to fail the job if quality score is below threshold'
    required: false
    default: 'true'

  output-format:
    description: 'Output format for analysis results (json, csv, text). JSON recommended for detailed export.'
    required: false
    default: 'json'

  export-json-path:
    description: 'Optional path to export detailed JSON results for downstream processing'
    required: false
    default: ''

  dataprof-version:
    description: 'Version of dataprof to use (latest, or specific version like v0.4.77)'
    required: false
    default: 'latest'

outputs:
  quality-score:
    description: 'Overall data quality score (0-100)'
    value: ${{ steps.quality-analysis.outputs.quality-score }}

  quality-level:
    description: 'Quality level classification (EXCELLENT, GOOD, FAIR, POOR)'
    value: ${{ steps.quality-analysis.outputs.quality-level }}

  completeness-score:
    description: 'Data completeness score (0-100)'
    value: ${{ steps.quality-analysis.outputs.completeness-score }}

  uniqueness-score:
    description: 'Data uniqueness score (0-100)'
    value: ${{ steps.quality-analysis.outputs.uniqueness-score }}

  validity-score:
    description: 'Data validity score (0-100)'
    value: ${{ steps.quality-analysis.outputs.validity-score }}

  consistency-score:
    description: 'Data consistency score (0-100)'
    value: ${{ steps.quality-analysis.outputs.consistency-score }}

  timeliness-score:
    description: 'Data timeliness score (0-100)'
    value: ${{ steps.quality-analysis.outputs.timeliness-score }}

  accuracy-score:
    description: 'Data accuracy score (0-100)'
    value: ${{ steps.quality-analysis.outputs.accuracy-score }}

  issues-count:
    description: 'Total number of quality issues detected'
    value: ${{ steps.quality-analysis.outputs.issues-count }}

  file-path:
    description: 'Path of the analyzed file(s)'
    value: ${{ steps.quality-analysis.outputs.file-path }}

  files-analyzed:
    description: 'Number of files analyzed (useful in batch mode)'
    value: ${{ steps.quality-analysis.outputs.files-analyzed }}

  json-export-path:
    description: 'Path to exported JSON file (if export-json-path was specified)'
    value: ${{ steps.quality-analysis.outputs.json-export-path }}

  batch-summary:
    description: 'Summary of batch processing results (when batch-mode is enabled)'
    value: ${{ steps.quality-analysis.outputs.batch-summary }}

  analysis-summary:
    description: 'Human-readable analysis summary'
    value: ${{ steps.quality-analysis.outputs.analysis-summary }}

runs:
  using: 'composite'
  steps:
  - name: Validate inputs
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Validating inputs"

      batch_mode="${{ inputs.batch-mode }}"
      file_input="${{ inputs.file }}"

      # Validate based on batch mode
      if [[ "$batch_mode" == "true" ]]; then
        echo "::notice::Batch mode enabled - validating directory/pattern"

        # Check if input is a directory or contains glob pattern
        if [[ -d "$file_input" ]]; then
          echo "✅ Directory input detected: $file_input"
        elif [[ "$file_input" == *"*"* ]] || [[ "$file_input" == *"?"* ]]; then
          echo "✅ Glob pattern detected: $file_input"
        else
          # Single file in batch mode is still valid
          if [[ ! -f "$file_input" ]]; then
            echo "::error::In batch mode, input must be a directory, glob pattern, or existing file"
            exit 1
          fi
          echo "✅ Single file in batch mode: $file_input"
        fi
      else
        echo "::notice::Single file mode - validating file"

        # Validate file existence and readability
        if [[ ! -f "$file_input" ]]; then
          echo "::error file=$file_input::File not found"
          exit 1
        fi

        if [[ ! -r "$file_input" ]]; then
          echo "::error file=$file_input::File is not readable"
          exit 1
        fi

        # Check file extension and format support
        if [[ "$file_input" =~ \.(csv|CSV|json|JSON|parquet|PARQUET)$ ]]; then
          echo "✅ Supported file format detected"
        else
          echo "::warning file=$file_input::File extension not recognized. Supported: .csv, .json, .parquet"
        fi
      fi

      # Validate threshold is numeric and in range
      threshold="${{ inputs.quality-threshold }}"
      if ! [[ "$threshold" =~ ^[0-9]+$ ]] || [[ "$threshold" -lt 0 ]] || [[ "$threshold" -gt 100 ]]; then
        echo "::error::quality-threshold must be a number between 0 and 100, got: $threshold"
        exit 1
      fi

      # Validate output format
      format="${{ inputs.output-format }}"
      if [[ ! "$format" =~ ^(json|csv|text)$ ]]; then
        echo "::error::output-format must be one of: json, csv, text. Got: $format"
        exit 1
      fi

      echo "Input validation passed"
      echo "::endgroup::"

  - name: Setup dataprof
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Setting up dataprof"

      # Determine version to install
      VERSION="${{ inputs.dataprof-version }}"
      if [[ "$VERSION" == "latest" ]] || [[ -z "$VERSION" ]]; then
        # Query crates.io API for latest version
        VERSION=$(curl -s https://crates.io/api/v1/crates/dataprof | jq -r '.crate.max_version')
      fi

      echo "::notice::Installing dataprof version: $VERSION"

      # Setup Rust environment if needed
      if ! command -v cargo &> /dev/null; then
        echo "::group::Installing Rust toolchain"
        curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --quiet
        echo "$HOME/.cargo/bin" >> "$GITHUB_PATH"
        source "$HOME/.cargo/env"
        echo "::endgroup::"
      fi

      # Install dataprof with proper error handling
      echo "::group::Installing dataprof via cargo"

      if ! cargo install dataprof --version "=$VERSION" --features minimal --locked; then
        echo "::warning::Failed to install with lockfile, retrying without..."
        if ! cargo install dataprof --version "=$VERSION" --features minimal; then
          echo "::warning::Failed to install specific version, trying latest..."
          if ! cargo install dataprof --features minimal; then
            echo "::error::All installation attempts failed"
            exit 1
          fi
        fi
      fi

      echo "::endgroup::"

      # Verify installation - binary is named 'dataprof-cli'
      if command -v dataprof-cli &> /dev/null; then
        echo "✅ dataprof-cli installed successfully"
        # Add cargo bin to PATH for subsequent steps
        echo "$HOME/.cargo/bin" >> "$GITHUB_PATH"

        INSTALLED_BIN=$(which dataprof-cli)
        echo "::notice::Binary location: $INSTALLED_BIN"

        # Show version
        dataprof-cli --version || echo "::warning::Could not get version"
      else
        echo "::error::dataprof-cli installation verification failed"
        echo "::notice::Checking for alternative binary names..."
        command -v dataprof && echo "Found 'dataprof' binary" || echo "No dataprof binary found"
        ls -la "$HOME/.cargo/bin/" 2>/dev/null || echo "Cargo bin directory not accessible"
        exit 1
      fi

      echo "::endgroup::"

  - name: Run data quality analysis
    id: quality-analysis
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Running data quality analysis"

      # Setup temporary workspace with cleanup trap
      temp_dir=$(mktemp -d)
      output_file="$temp_dir/quality_analysis.${{ inputs.output-format }}"

      cleanup() {
        [[ -d "$temp_dir" ]] && rm -rf "$temp_dir"
      }
      trap cleanup EXIT

      # Determine dataprof binary name
      if [[ "$RUNNER_OS" == "Windows" ]] || [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]]; then
        DATAPROF_BIN="dataprof-cli.exe"
      else
        DATAPROF_BIN="dataprof-cli"
      fi

      batch_mode="${{ inputs.batch-mode }}"
      file_input="${{ inputs.file }}"
      export_path="${{ inputs.export-json-path }}"

      # Build command based on mode
      if [[ "$batch_mode" == "true" ]]; then
        echo "::notice::Running in BATCH mode"

        # Build batch command
        cmd_args=("batch")

        # Handle directory vs glob pattern
        if [[ -d "$file_input" ]]; then
          cmd_args+=("$file_input")
        else
          # For glob patterns, we need to expand them
          # Use the parent directory and let dataprof handle filtering
          parent_dir=$(dirname "$file_input")
          if [[ -d "$parent_dir" ]]; then
            cmd_args+=("$parent_dir")
          else
            cmd_args+=(".")
          fi
        fi

        # Add batch-specific flags
        [[ "${{ inputs.recursive }}" == "true" ]] && cmd_args+=("--recursive")
        [[ "${{ inputs.parallel }}" == "true" ]] && cmd_args+=("--parallel")

        # Output format
        cmd_args+=("--format" "${{ inputs.output-format }}")

        # For JSON, always add detailed flag
        if [[ "${{ inputs.output-format }}" == "json" ]]; then
          cmd_args+=("--detailed")
        fi

      else
        echo "::notice::Running in SINGLE FILE mode"

        # Build analyze command
        cmd_args=(
          "analyze"
          "$file_input"
          "--format" "${{ inputs.output-format }}"
        )

        # Add detailed flag for comprehensive output
        if [[ "${{ inputs.output-format }}" == "json" ]]; then
          cmd_args+=("--detailed")
        fi
      fi

      echo "::notice::Executing: $DATAPROF_BIN ${cmd_args[*]}"

      if ! "$DATAPROF_BIN" "${cmd_args[@]}" > "$output_file" 2>&1; then
        exit_code=$?
        echo "::error::Analysis failed with exit code $exit_code"
        echo "::group::Error output"
        cat "$output_file" 2>/dev/null || echo "No output available"
        echo "::endgroup::"
        exit $exit_code
      fi

      # Export JSON if requested
      if [[ -n "$export_path" ]] && [[ "${{ inputs.output-format }}" == "json" ]]; then
        echo "::notice::Exporting JSON to $export_path"
        mkdir -p "$(dirname "$export_path")"
        cp "$output_file" "$export_path"
        echo "JSON_EXPORT_PATH=$export_path" >> "$GITHUB_ENV"
      fi

      echo "Analysis completed successfully"
      echo "::endgroup::"

      echo "::group::Parsing analysis results"

      batch_mode="${{ inputs.batch-mode }}"
      files_analyzed="1"
      batch_summary=""

      # Parse based on output format
      if [[ "${{ inputs.output-format }}" == "json" ]]; then
        # Ensure jq is available (GitHub runners have it pre-installed)
        if ! command -v jq &> /dev/null; then
          echo "::error::jq is required for JSON parsing but not available"
          exit 1
        fi

        # Validate JSON format
        if ! jq empty "$output_file" 2>/dev/null; then
          echo "::error::Invalid JSON format in output file"
          exit 1
        fi

        # Check if this is batch output (array) or single file output (object)
        if [[ "$batch_mode" == "true" ]] && jq -e 'type == "array"' "$output_file" >/dev/null 2>&1; then
          echo "::notice::Parsing BATCH mode results"

          # Count files analyzed
          files_analyzed=$(jq 'length' "$output_file" 2>/dev/null || echo "0")

          # Aggregate metrics across all files
          # Average quality score
          quality_score=$(jq '[.[].quality.quality_score // .[].summary.data_quality_score // 0] | add / length' "$output_file" 2>/dev/null || echo "0")

          # Aggregate dimension scores
          completeness=$(jq '[.[].data_quality_metrics.completeness.complete_records_ratio // 0] | add / length' "$output_file" 2>/dev/null || echo "0")
          uniqueness=$(jq '[.[].data_quality_metrics.uniqueness.key_uniqueness // 0] | add / length' "$output_file" 2>/dev/null || echo "0")
          consistency=$(jq '[.[].data_quality_metrics.consistency.data_type_consistency // 0] | add / length' "$output_file" 2>/dev/null || echo "0")

          # Accuracy: average of (100 - outlier_ratio)
          outlier_avg=$(jq '[.[].data_quality_metrics.accuracy.outlier_ratio // 0] | add / length' "$output_file" 2>/dev/null || echo "0")
          accuracy=$(awk -v outlier="$outlier_avg" 'BEGIN { printf "%.2f", 100 - outlier }')

          # Timeliness: average if available
          if jq -e '.[0].data_quality_metrics.timeliness' "$output_file" >/dev/null 2>&1; then
            stale_avg=$(jq '[.[].data_quality_metrics.timeliness.stale_data_ratio // 0] | add / length' "$output_file" 2>/dev/null || echo "0")
            timeliness=$(awk -v stale="$stale_avg" 'BEGIN { printf "%.2f", 100 - stale }')
          else
            timeliness="100.0"
          fi

          validity="$consistency"

          # Total issues across all files
          duplicate_rows=$(jq '[.[].data_quality_metrics.uniqueness.duplicate_rows // 0] | add' "$output_file" 2>/dev/null || echo "0")
          format_violations=$(jq '[.[].data_quality_metrics.consistency.format_violations // 0] | add' "$output_file" 2>/dev/null || echo "0")
          range_violations=$(jq '[.[].data_quality_metrics.accuracy.range_violations // 0] | add' "$output_file" 2>/dev/null || echo "0")
          issues_count=$(awk -v dup="$duplicate_rows" -v fmt="$format_violations" -v range="$range_violations" 'BEGIN { print dup + fmt + range }')

          file_path="${{ inputs.file }}"

          # Create batch summary
          batch_summary="Analyzed $files_analyzed files | Avg Quality: ${quality_score}% | Total Issues: $issues_count"

          echo "Batch JSON results parsed successfully"

        else
          echo "::notice::Parsing SINGLE FILE mode results"

          # Extract metrics from ACTUAL dataprof JSON structure
          # Overall quality score
          quality_score=$(jq -r '.quality.quality_score // .summary.data_quality_score // "0"' "$output_file" 2>/dev/null || echo "0")

          # Extract individual dimension scores from data_quality_metrics
          completeness=$(jq -r '.data_quality_metrics.completeness.complete_records_ratio // "0"' "$output_file" 2>/dev/null || echo "0")
          uniqueness=$(jq -r '.data_quality_metrics.uniqueness.key_uniqueness // "0"' "$output_file" 2>/dev/null || echo "0")
          consistency=$(jq -r '.data_quality_metrics.consistency.data_type_consistency // "0"' "$output_file" 2>/dev/null || echo "0")

          # Accuracy score (100 - outlier_ratio)
          outlier_ratio=$(jq -r '.data_quality_metrics.accuracy.outlier_ratio // "0"' "$output_file" 2>/dev/null || echo "0")
          accuracy=$(awk -v outlier="$outlier_ratio" 'BEGIN { printf "%.2f", 100 - outlier }')

          # Timeliness score - Check if timeliness metrics exist in JSON
          if jq -e '.data_quality_metrics.timeliness' "$output_file" >/dev/null 2>&1; then
            stale_ratio=$(jq -r '.data_quality_metrics.timeliness.stale_data_ratio // "0"' "$output_file" 2>/dev/null || echo "0")
            timeliness=$(awk -v stale="$stale_ratio" 'BEGIN { printf "%.2f", 100 - stale }')
          else
            timeliness="100.0"
          fi

          validity="$consistency"

          # Count issues from all dimensions
          duplicate_rows=$(jq -r '.data_quality_metrics.uniqueness.duplicate_rows // "0"' "$output_file" 2>/dev/null || echo "0")
          format_violations=$(jq -r '.data_quality_metrics.consistency.format_violations // "0"' "$output_file" 2>/dev/null || echo "0")
          range_violations=$(jq -r '.data_quality_metrics.accuracy.range_violations // "0"' "$output_file" 2>/dev/null || echo "0")
          issues_count=$(awk -v dup="$duplicate_rows" -v fmt="$format_violations" -v range="$range_violations" 'BEGIN { print dup + fmt + range }')

          file_path="${{ inputs.file }}"

          echo "Single file JSON results parsed successfully"
        fi

        # Calculate quality level from score (common for both modes)
        if awk -v score="$quality_score" 'BEGIN { exit !(score >= 90) }'; then
          quality_level="EXCELLENT"
        elif awk -v score="$quality_score" 'BEGIN { exit !(score >= 75) }'; then
          quality_level="GOOD"
        elif awk -v score="$quality_score" 'BEGIN { exit !(score >= 60) }'; then
          quality_level="FAIR"
        else
          quality_level="POOR"
        fi

      else
        # Basic text parsing for non-JSON formats
        if grep -q "Overall Quality Score:" "$output_file"; then
          quality_score=$(grep "Overall Quality Score:" "$output_file" | grep -o '[0-9]*\.*[0-9]*' | head -1 || echo "0")
          quality_level=$(grep "Quality Level:" "$output_file" | awk '{print $NF}' | head -1 || echo "UNKNOWN")
        else
          quality_score="0"
          quality_level="UNKNOWN"
        fi

        # Set defaults for unsupported metrics in text format
        completeness="0"
        uniqueness="0"
        validity="0"
        consistency="0"
        timeliness="0"
        accuracy="0"
        issues_count="0"
        file_path="${{ inputs.file }}"

        echo "Text results parsed"
      fi

      echo "::endgroup::"

      echo "::group::Setting outputs and environment variables"

      # Validate extracted metrics
      if [[ ! "$quality_score" =~ ^[0-9]+(\.[0-9]+)?$ ]]; then
        echo "::warning::Invalid quality score format: $quality_score, defaulting to 0"
        quality_score="0"
      fi

      # Create structured analysis summary
      if [[ "$batch_mode" == "true" ]]; then
        analysis_summary="Data Quality Analysis Results (Batch Mode):
      Files Analyzed: ${files_analyzed}
      Overall Score: ${quality_score}% (averaged)
      Quality Level: ${quality_level}
      Completeness: ${completeness}%
      Uniqueness: ${uniqueness}%
      Validity: ${validity}%
      Consistency: ${consistency}%
      Timeliness: ${timeliness}%
      Accuracy: ${accuracy}%
      Total Issues Found: ${issues_count}
      Input: ${file_path}"
      else
        analysis_summary="Data Quality Analysis Results:
      Overall Score: ${quality_score}%
      Quality Level: ${quality_level}
      Completeness: ${completeness}%
      Uniqueness: ${uniqueness}%
      Validity: ${validity}%
      Consistency: ${consistency}%
      Timeliness: ${timeliness}%
      Accuracy: ${accuracy}%
      Issues Found: ${issues_count}
      File: ${file_path}"
      fi

      # Get JSON export path from environment if set
      json_export="${JSON_EXPORT_PATH:-}"

      # Set GitHub outputs safely
      {
        echo "quality-score=$quality_score"
        echo "quality-level=$quality_level"
        echo "completeness-score=$completeness"
        echo "uniqueness-score=$uniqueness"
        echo "validity-score=$validity"
        echo "consistency-score=$consistency"
        echo "timeliness-score=$timeliness"
        echo "accuracy-score=$accuracy"
        echo "issues-count=$issues_count"
        echo "file-path=$file_path"
        echo "files-analyzed=$files_analyzed"
        echo "json-export-path=$json_export"
      } >> "$GITHUB_OUTPUT"

      # Set multiline outputs using heredoc syntax
      {
        echo "analysis-summary<<EOF_SUMMARY"
        echo "$analysis_summary"
        echo "EOF_SUMMARY"
      } >> "$GITHUB_OUTPUT"

      # Batch summary (if applicable)
      if [[ -n "$batch_summary" ]]; then
        {
          echo "batch-summary<<EOF_BATCH"
          echo "$batch_summary"
          echo "EOF_BATCH"
        } >> "$GITHUB_OUTPUT"
      else
        echo "batch-summary=" >> "$GITHUB_OUTPUT"
      fi

      # Store for threshold validation
      {
        echo "QUALITY_SCORE=$quality_score"
        echo "THRESHOLD=${{ inputs.quality-threshold }}"
      } >> "$GITHUB_ENV"

      echo "::endgroup::"

      # Display summary with GitHub annotations
      echo "::notice title=Analysis Complete::Quality Score: ${quality_score}% (Threshold: ${{ inputs.quality-threshold }}%)"

  - name: Quality gate validation
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Quality gate validation"

      # Extract values from environment with defaults
      quality_score=${QUALITY_SCORE:-0}
      threshold=${THRESHOLD:-80}
      fail_on_issues="${{ inputs.fail-on-issues }}"

      echo "::notice::Validating quality score: ${quality_score}% against threshold: ${threshold}%"

      # Robust floating point comparison using awk
      if awk -v score="$quality_score" -v thresh="$threshold" 'BEGIN { exit !(score >= thresh) }'; then
        echo "::notice title=Quality Gate Passed::Quality score (${quality_score}%) meets threshold (${threshold}%)"
        echo "Quality gate validation successful"
      else
        message="Quality score (${quality_score}%) below threshold (${threshold}%)"

        if [[ "$fail_on_issues" == "true" ]]; then
          echo "::error title=Quality Gate Failed::$message"
          echo "Quality gate validation failed - stopping workflow"
          exit 1
        else
          echo "::warning title=Quality Gate Warning::$message"
          echo "Quality gate warning - continuing workflow (fail-on-issues=false)"
        fi
      fi

      echo "::endgroup::"

  - name: Generate workflow summary
    shell: bash
    run: |
      set -euo pipefail

      echo "::group::Generating workflow summary"

      # Determine threshold status
      quality_score="${{ steps.quality-analysis.outputs.quality-score }}"
      threshold="${{ inputs.quality-threshold }}"
      threshold_status="FAILED"

      if awk -v score="$quality_score" -v thresh="$threshold" 'BEGIN { exit !(score >= thresh) }'; then
        threshold_status="PASSED"
      fi

      # Generate comprehensive summary
      output_format="${{ inputs.output-format }}"
      dataprof_version="${{ inputs.dataprof-version }}"
      fail_on_issues="${{ inputs.fail-on-issues }}"
      issues_count="${{ steps.quality-analysis.outputs.issues-count }}"
      quality_level="${{ steps.quality-analysis.outputs.quality-level }}"
      file_path="${{ steps.quality-analysis.outputs.file-path }}"
      files_analyzed="${{ steps.quality-analysis.outputs.files-analyzed }}"
      batch_mode="${{ inputs.batch-mode }}"
      json_export="${{ steps.quality-analysis.outputs.json-export-path }}"

      next_steps="- Excellent! Your data quality is high
      - No critical issues detected
      - Consider monitoring data quality over time"

      if [[ "$issues_count" != "0" ]]; then
        next_steps="- Review the detailed quality metrics in the action outputs
        - Address the ${issues_count} issue(s) identified
        - Run analysis again after applying improvements"
      fi

      {
        echo "# DataProf Data Quality Analysis"
        echo ""

        # Add batch mode indicator if applicable
        if [[ "$batch_mode" == "true" ]]; then
          echo "> **Batch Mode** | Analyzed **${files_analyzed}** file(s)"
          echo ""
        fi

        echo "## Results Summary"
        echo ""
        echo "| Metric | Score |"
        echo "|--------|-------|"
        echo "| **Overall Quality** | ${quality_score}% (${quality_level}) |"
        echo "| **Completeness** | ${{ steps.quality-analysis.outputs.completeness-score }}% |"
        echo "| **Uniqueness** | ${{ steps.quality-analysis.outputs.uniqueness-score }}% |"
        echo "| **Validity** | ${{ steps.quality-analysis.outputs.validity-score }}% |"
        echo "| **Consistency** | ${{ steps.quality-analysis.outputs.consistency-score }}% |"
        echo "| **Timeliness** | ${{ steps.quality-analysis.outputs.timeliness-score }}% |"
        echo "| **Accuracy** | ${{ steps.quality-analysis.outputs.accuracy-score }}% |"
        echo ""
        echo "## Quality Gate"
        echo ""
        echo "- **Threshold**: ${threshold}%"
        echo "- **Result**: ${threshold_status}"
        echo "- **Issues Found**: ${issues_count}"
        echo "- **Fail on Issues**: ${fail_on_issues}"
        echo ""
        echo "## Analysis Details"
        echo ""

        if [[ "$batch_mode" == "true" ]]; then
          echo "- **Mode**: Batch Processing"
          echo "- **Files Analyzed**: ${files_analyzed}"
          echo "- **Input Pattern**: \`${file_path}\`"
        else
          echo "- **Mode**: Single File"
          echo "- **File**: \`${file_path}\`"
        fi

        echo "- **Output Format**: ${output_format}"
        echo "- **DataProf Version**: ${dataprof_version}"

        # Show JSON export path if available
        if [[ -n "$json_export" ]]; then
          echo "- **JSON Export**: \`${json_export}\`"
        fi

        echo ""
        echo "## Next Steps"
        echo ""
        echo "${next_steps}"
        echo ""
        echo "---"
        echo "*Powered by [DataProf v${dataprof_version}](https://github.com/AndreaBozzo/dataprof) • Generated $(date -u '+%Y-%m-%d %H:%M:%S UTC')*"
      } >> "$GITHUB_STEP_SUMMARY"

      echo "Workflow summary generated"
      echo "::endgroup::"